{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"jumbotron m-0\">\n",
    "    <hr />\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Drone SIMULATION\n",
    "</span>\n",
    "    </h1>\n",
    "    <hr />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"bg-primary text-center\">\n",
    "    - Summary -\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1.0 Environment](#Environment)\n",
    "* [1.1 Drone](#Drone-in-Gallery-Room)\n",
    "* [1.2 Controlling the Drone](#Controlling-the-Drone)\n",
    "* [1.3 Drone Camera](#Drone-Camera)\n",
    "* [1.4 Drone Velodyne](#Drone-Velodyne)\n",
    "* [1.5 3D Mapping Using LIDAR and IMU](#3D-Mapping-Using-LIDAR-and-IMU)\n",
    "* [1.6 Other Sensors](#Other-Sensors)\n",
    "* [1.7 Sources and Thanks](#Thanks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"bg-primary text-center\">\n",
    "    - End of Summary -\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sim'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/demo1_simulation.png\"/>\n",
    "\n",
    "**Drone Gallery Demo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is a simulation of a quadcopter drone.\n",
    "* The drone is based on the Sjtu drone.\n",
    "* The quadcopter design is robust and agile, equipped with advanced sensors that enable it to perform precise and complex movements within the art gallery.\n",
    "* It is placed in an art gallery environment, which includes many walls, lights, and seats, creating a realistic and challenging scenario for navigation and mapping tasks.\n",
    "* There are three Apriltags in the environment, one on the floor and two on the walls.\n",
    "* These Apriltags are used to scan with the drone's camera to perform specific tasks.\n",
    "* The simulation integrates the `lio_sam` package for mapping, which combines LIDAR and IMU data to create an accurate 3D map of the room, essential for navigation and exploration.\n",
    "* The teleop package is utilized for manual control, allowing the user to navigate the drone through the art gallery, control its speed, and perform various maneuvers efficiently and safely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demos'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drone in Gallery Room"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this simulation, we utilize a quadcopter drone equipped with advanced sensors and navigation systems.\n",
    "* The environment is an art gallery with many walls, lights, and seats, creating a realistic and complex scenario for the drone.\n",
    "* The goal of the quadcopter drone is to navigate through the art gallery, identify and interact with specific Apriltags located in different parts of the gallery. Using the integrated sensors and ROS2 for control, the drone approaches each Apriltag, scans it, and performs predefined tasks based on the detection.\n",
    "* Also the simulation integrates the lio_sam package for mapping, which combines LIDAR and IMU data to create an accurate 3D map of the room, essential for navigation and exploration.\n",
    "* The sensors available for the drone are:\n",
    "    * Front camera: For forward-facing vision tasks, aiding in navigation and obstacle detection.\n",
    "    * Downlooking camera: For ground observation, assisting in precise landing and floor-level navigation.\n",
    "    * Velodyne LIDAR: For detailed 3D mapping of the environment, essential for accurate navigation and obstacle avoidance.\n",
    "    * Sonar: For close-range obstacle detection, enhancing safety during flight.\n",
    "    * IMU (Inertial Measurement Unit): For stable flight and precise motion tracking, ensuring accurate navigation and maneuvering.\n",
    "    * GPS sensor: For global positioning, aiding in overall navigation and providing accurate location data.\n",
    "* Upon launching the simulation in Gazebo, you'll observe the following setup:\n",
    "The quadcopter drone is positioned at the starting point in the art gallery, ready to begin its mission of scanning Apriltags and mapping the environment using its advanced sensor suite. The gallery is designed to challenge the drone with various obstacles and navigation tasks, making it an ideal testbed for autonomous flight and mapping capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/demo1_start.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can use the lio_sam package, you need to compile its dependencies. One of these dependencies is the GTSAM (Georgia Tech Smoothing and Mapping) library. Follow the steps below to compile GTSAM:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd gtsam\n",
    "mkdir build\n",
    "cd /home/simulations/ros2_sims_ws/src/drones_ROS2/gtsam/build\n",
    "cmake ..\n",
    "make -j4\n",
    "sudo make install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AprilTags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriltags are visual fiducial markers used for accurate localization and identification within the environment. In this simulation, three Apriltags are strategically placed in the art gallery:\n",
    "- One on the floor.\n",
    "- One at the end of the left hall.\n",
    "- One at the end of the right hall.\n",
    "\n",
    "These Apriltags are used by the drone's cameras to perform specific tasks, such as navigation, localization, and triggering predefined actions. By scanning these tags, the drone can accurately determine its position and execute corresponding commands. These are located in red in the following image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/demo1_atags.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling the Drone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The teleop package provides a simple way to manually control the drone using keyboard inputs. This package translates keyboard commands into velocity commands that can be sent to the drone, allowing for intuitive and straightforward control. Below are the instructions to take off, control the drone using the keyboard, and land the drone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Takeoff Command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initiate the takeoff sequence for the drone, use the following command. This command publishes an empty message to the `/drone/takeoff` topic, instructing the drone to take off. You should expect the following behavior:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ros2 topic pub /drone/takeoff std_msgs/msg/Empty {} --once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"images/takeoff.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Teleop Command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Once the drone is airborne, you can control its movements using the teleop package. The command below launches the teleop_twist_keyboard node, which allows you to control the drone using your keyboard. The velocity commands are remapped to the `/drone/cmd_vel` topic."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ros2 run teleop_twist_keyboard teleop_twist_keyboard --ros-args --remap cmd_vel:=/drone/cmd_vel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After the command you can control the robot with the following keys:\n",
    "\n",
    "---------------------------\n",
    "Moving around:\n",
    "   u    i    o\n",
    "   j    k    l\n",
    "   m    ,    .\n",
    "\n",
    "For Holonomic mode (strafing), hold down the shift key:\n",
    "\n",
    "---------------------------\n",
    "   U    I    O\n",
    "   J    K    L\n",
    "   M    <    >\n",
    "\n",
    "t : up (+z)\n",
    "b : down (-z)\n",
    "\n",
    "anything else : stop\n",
    "\n",
    "q/z : increase/decrease max speeds by 10%\n",
    "w/x : increase/decrease only linear speed by 10%\n",
    "e/c : increase/decrease only angular speed by 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/teleop_drone.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Landing Command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To safely land the drone, use the following command. This command publishes an empty message to the `/drone/land` topic, instructing the drone to land. You should expect the following behavior:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ros2 topic pub /drone/land std_msgs/msg/Empty {} --once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"images/landing.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drone Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadcopter drone is equipped with two cameras: a front camera and a downlooking camera. These cameras play a crucial role in navigation, obstacle detection, and task execution. The front camera provides a forward-facing view, essential for navigating through the environment, while the downlooking camera offers a view of the ground, aiding in precise landing and ground-level inspections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Front Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The front camera is used for forward-facing vision tasks. It helps the drone navigate through the environment by providing real-time video feed and image data. For example, placing the Drone as the following image we should see a painting by Velazquez:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/camera1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the front camera feed in RViz, add an Image display and set the topic to `/drone/front/image_raw` or run the following command to see both cameras. This will allow you to see what the drone's front camera sees."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rviz2 -d home/user/drone_camera.rviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/camera2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The downlooking camera provides a view of the ground directly beneath the drone. This is particularly useful for tasks that require ground observation, such as precise landing and identifying objects or markers on the floor. For example, if we place the Drone above the AprilTag on the floor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/camera3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the downlooking camera feed in RViz, add an Image display and set the topic to `/drone/bottom/image_raw` or run the same command as before. This will allow you to see what the drone's downlooking camera sees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/camera4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drone Velodyne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadcopter drone is equipped with a Velodyne LIDAR sensor, which is essential for creating detailed 3D maps of the environment. The Velodyne LIDAR emits laser pulses and measures the time it takes for the pulses to return after hitting an object. This data is used to create a precise 3D representation of the surroundings, which is crucial for navigation, obstacle avoidance, and environmental mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic where the LIDAR data is published is `/points_raw`. This topic provides point cloud data, which can be visualized in RViz or used for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Mapping Using LIDAR and IMU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMU (Inertial Measurement Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Inertial Measurement Unit (IMU) is a crucial sensor on the drone, providing data on its orientation, acceleration, and angular velocity. This information is essential for maintaining stable flight and accurate motion tracking. The IMU data is published to the `/drone/imu/out` topic, which can be used in conjunction with LIDAR data for precise mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating LIDAR and IMU Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LIDAR data is integrated with the drone's IMU (Inertial Measurement Unit) data to create an accurate and detailed 3D map of the environment. This is achieved using the lio_sam package, which combines LIDAR and IMU data for robust and precise mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lio_sam` package processes the data from the Velodyne LIDAR and the IMU to build a 3D map, which can be used for navigation, exploration, and obstacle avoidance. By continuously updating the map as the drone moves, it ensures that the drone has an up-to-date understanding of its surroundings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform 3D mapping using the `lio_sam` package, you need to launch the necessary nodes. These nodes will process the LIDAR and IMU data to continuously build and update the 3D map as the drone navigates through the environment. Running the following command:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ros2 launch sjtu_drone_bringup start_lio_sam.launch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, this command will launch RViz2 with the appropriate configuration settings. You should see a display similar to the following image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3d_mapping.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time 3D Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the drone navigates through the environment, the `lio_sam` package continuously generates and updates the 3D map. This real-time mapping allows the drone to have an up-to-date understanding of its surroundings, which is crucial for autonomous navigation and obstacle avoidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also refer to this video to see the 3D mapping in action as the drone navigates through the environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3d_mapping.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Saving the 3D Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "Once the 3D mapping process is complete, it is important to save the generated map for future use, analysis, and navigation. The lio_sam package allows you to save the detailed 3D map created during the mapping process. This map can be used to plan routes, identify obstacles, and provide a detailed understanding of the environment.\n",
    "\n",
    "#### Features of the Saved Map\n",
    "- High Resolution: The map is saved with a specified resolution, providing detailed and accurate representations of the environment.\n",
    "- Reusable Data: The saved map can be reloaded and used in different sessions, allowing for continuous and consistent analysis.\n",
    "- Enhanced Navigation: The map aids in planning efficient navigation paths, avoiding obstacles, and understanding the spatial layout.\n",
    "\n",
    "#### Command to Save the Map\n",
    "To save the 3D map, you can use the ROS 2 service call command. This command interacts with the `lio_sam/save_map` service, specifying the resolution and destination directory for the saved map. For example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "ros2 service call /lio_sam/save_map lio_sam/srv/SaveMap \"{resolution: 0.2, destination: '/Downloads/LOAM/'}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the Velodyne LIDAR and IMU, the quadcopter drone is equipped with Sonar and GPS sensors. These sensors play important roles in enhancing the drone's navigation and environmental awareness capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sonar Sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sonar sensor is used for close-range obstacle detection. It emits sound waves and measures the time it takes for the echoes to return after hitting an object. This information helps the drone detect obstacles in its immediate vicinity, enhancing safety during flight.\n",
    "\n",
    "The data from the Sonar sensor is published to the `/drone/sonar` topic. This topic provides distance measurements to nearby objects, which can be used for obstacle avoidance and precise navigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GPS Sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPS (Global Positioning System) sensor provides accurate location data, which is essential for global navigation and positioning. The GPS sensor allows the drone to determine its exact position in the environment, aiding in route planning and long-distance navigation.\n",
    "\n",
    "The GPS data is published to the `/drone/gps` topic. This topic includes information about the drone's latitude, longitude, and altitude, enabling it to navigate effectively over large distances and in complex environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ROS Version\n",
    "    * This simulation has been tested in `ROS2 Humble`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='thanks'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This course would not have been possible without the knowledge and work of the [ROS Community](http://www.ros.org/), [OSRF](https://www.osrfoundation.org/), and [Gazebo Team](http://gazebosim.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/osrf_logo.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Thanks to [BLENDER](https://www.blender.org/) for their amaizing tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/blender.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/English-Proofread-Pending-TCS.png\" width=\"200\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
